# -*- coding: utf-8 -*-
"""NER_easy_fineTuning_NERDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HB_wxa0QnBW38y7Pk7s2UoFqDDMZcvPw
"""

!pip install NERDA
from google.colab import drive
import os
import pandas as pd
drive.mount('/content/drive')

from NERDA.datasets import get_conll_data, download_conll_data 
download_conll_data()
training = get_conll_data('train')
validation = get_conll_data('valid')
tag_scheme = [
'B-PER',
'I-PER',
'B-ORG',
'I-ORG',
'B-LOC',
'I-LOC',
'B-MISC',
'I-MISC'
]

training.items()

for key in training:
  print(key, '->', training[key])

path = '/content/drive/MyDrive/colab_data'
data = pd.read_csv(os.path.join(path,'ner_dataset.csv'), encoding= 'unicode_escape')
data.columns = ['sentence', 'word', 'pos', 'tag']
data.ffill(inplace=True)
data.head()

class getsentence(object):
    
    def __init__(self, data):
        self.n_sent = 1.0
        self.data = data
        self.empty = False
        word_agg_func = lambda s: [w for w in s["word"].values.tolist()]
        tag_agg_func = lambda s: [w for w in s["tag"].values.tolist()]
        self.grouped_word = self.data.groupby("sentence").apply(word_agg_func)
        self.sentences = [s for s in self.grouped_word]
        self.grouped_tag = self.data.groupby("sentence").apply(tag_agg_func)
        self.tags = [t for t in self.grouped_tag]
        
getter = getsentence(data.head(10000))
sentences = getter.sentences
tags = getter.tags
training = {'sentences': sentences, 'tags': tags}

getter = getsentence(data.tail(3000))
sentences = getter.sentences
tags = getter.tags
validation = {'sentences': sentences, 'tags': tags}

data.head(5000).tag.unique()

tag_scheme = ['B-geo', 'I-geo',
              'B-gpe', 'I-gpe', 
              'B-per', 'I-per',               
              'B-org', 'I-org',               
              'B-tim', 'I-tim',
              'B-art', 'I-art',                
              'B-nat', #'I-nat',
              'B-eve', 'I-eve'              
              ]



transformer = 'bert-base-multilingual-uncased'

# hyperparameters for network
dropout = 0.1
# hyperparameters for training
training_hyperparameters = {
'epochs' : 4,
'warmup_steps' : 500,                                                   'train_batch_size': 13,                                         'learning_rate': 0.0001
}

from NERDA.models import NERDA
model = NERDA(
dataset_training = training,
dataset_validation = validation,
tag_scheme = tag_scheme, 
tag_outside = 'O',
transformer = transformer,
dropout = dropout,
hyperparameters = training_hyperparameters
)

model.train()

import nltk
nltk.download('punkt')

model.predict_text('Prime Minister Jacinda Ardern has claimed that New Zealand had won a big battle over the spread of coronavirus. Her words came as the country begins to exit from its lockdown')