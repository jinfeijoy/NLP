{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_fastai1_with_transformer_googlecolab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNerW6VWrOYHsaEgsMxCA+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinfeijoy/NLP/blob/main/kaggle_IMDB_Review/notebook/IMDB_fastai1_with_transformer_googlecolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zm4Mzx2heBJ",
        "outputId": "bf5079b0-b1fa-4992-a806-b334c0f64640"
      },
      "source": [
        "#Please select GPU first (from Edit->NotebookSetting)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "!pip install fastai==1.0.58\n",
        "# !pip install urllib3==1.25.4\n",
        "!pip install transformers==2.5.1\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "import transformers\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import random \n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai==1.0.58\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/88/386289f6926a59cbd2765b033f5fe8414d6ff89ab27044dffe740cc1a5f3/fastai-1.0.58-py3-none-any.whl (236kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (2.7.3)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (3.2.2)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (2.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (7.1.2)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (7.352.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (20.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (3.13)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (0.9.1+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.1.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (1.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.58) (4.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.58) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.58) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.58) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.58) (2.8.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.58) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.58) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.58) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.58) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.58) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->fastai==1.0.58) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==1.0.58) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai==1.0.58) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.58) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.58) (3.4.1)\n",
            "Installing collected packages: fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-1.0.58\n",
            "Collecting transformers==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2.23.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/77/cc19511d0fe4672890209ebcbfb9b3f4746572f5a48f7ed2654e7f8c2f29/boto3-1.17.89-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (3.0.4)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.89\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/e6e16f588036b387d4f4f8e2e780a520e2b3925ba1529fdfc97ad909fb6c/botocore-1.20.89-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.89->boto3->transformers==2.5.1) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.89 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, transformers\n",
            "Successfully installed boto3-1.17.89 botocore-1.20.89 jmespath-0.10.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.5.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnpFT7WmFswE"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "000Lxer88C6x"
      },
      "source": [
        "# Load Data\n",
        "Reference: https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB5kSJ-qh9JV"
      },
      "source": [
        "path = '/content/drive/MyDrive/colab_data'\n",
        "def de_emojify(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "def tweet_proc(df, text_col='text'):\n",
        "    df['orig_text'] = df[text_col]\n",
        "    # Remove twitter handles\n",
        "    df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
        "    # Remove URLs\n",
        "    df[text_col] = df[text_col].apply(lambda x:x.replace('<br />', ' '))\n",
        "    return df[df[text_col]!='']\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Eif24MQRQANx",
        "outputId": "9978ed14-61c8-4070-a52d-360556dc4a59"
      },
      "source": [
        "basic_tweet = pd.read_csv(os.path.join(path, \"IMDB_Dataset.csv\"))\n",
        "basic_tweet = basic_tweet[basic_tweet.sentiment!='empty'].drop_duplicates().sample(1000, random_state = 10).reset_index(drop=True)\n",
        "# basic_tweet = pd.read_csv(os.path.join(path, \"tweet_dataset.csv\"))\n",
        "# basic_tweet = basic_tweet[basic_tweet.sentiment!='empty'].drop_duplicates().reset_index(drop=True)\n",
        "# basic_tweet = basic_tweet[['sentiment','new_sentiment','old_text']].rename(columns={'old_text':'text', 'sentiment':'emotion', 'new_sentiment':'sentiment'})\n",
        "basic_tweet = tweet_proc(basic_tweet,'review').dropna(subset=['sentiment'])\n",
        "print(len(basic_tweet))\n",
        "basic_tweet.head(3)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>orig_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE TENANTS began as a 1971 short novel by the...</td>\n",
              "      <td>positive</td>\n",
              "      <td>THE TENANTS began as a 1971 short novel by the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Being a fan of the manga and anime of Go Nagai...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Being a fan of the manga and anime of Go Nagai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Elia Kazan, one of the best theater directors ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>Elia Kazan, one of the best theater directors ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                          orig_text\n",
              "0  THE TENANTS began as a 1971 short novel by the...  ...  THE TENANTS began as a 1971 short novel by the...\n",
              "1  Being a fan of the manga and anime of Go Nagai...  ...  Being a fan of the manga and anime of Go Nagai...\n",
              "2  Elia Kazan, one of the best theater directors ...  ...  Elia Kazan, one of the best theater directors ...\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R_Io_SR9yjD"
      },
      "source": [
        "# Main transformers classes\n",
        "* A **model class** to load/store a particular pre-train model.\n",
        "* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n",
        "* A **configuration class** to load/store the configuration of a particular model.\n",
        "\n",
        "Pre-trained model name can be found here: https://huggingface.co/transformers/pretrained_models.html#pretrained-models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IosPm09PBT53"
      },
      "source": [
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5gG7whd94Ez"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
        "}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CebDd2j7-xLR"
      },
      "source": [
        "seed = 42\n",
        "use_fp16 = False\n",
        "bs = 16\n",
        "\n",
        "# model_type = 'roberta'\n",
        "# pretrained_model_name = 'roberta-base'\n",
        "\n",
        "# model_type = 'bert'\n",
        "# pretrained_model_name='bert-base-uncased'\n",
        "\n",
        "model_type = 'distilbert'\n",
        "pretrained_model_name = 'distilbert-base-uncased'\n",
        "\n",
        "#model_type = 'xlm'\n",
        "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
        "\n",
        "# model_type = 'xlnet'\n",
        "# pretrained_model_name = 'xlnet-base-cased'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3TKgiIAIEx"
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7EoC-U5BjEA"
      },
      "source": [
        "# Function to set the seed for generating random numbers\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA0nSgbXBzdT"
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liNQGmNbCg22"
      },
      "source": [
        "# Data Pre-processing\n",
        "\n",
        "### Custom Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DboUvy0hB2Fx"
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        # self.max_seq_len = int(np.percentile(pretrained_tokenizer.max_len, 95))\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "            tokens = [CLS] + tokens + [SEP]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "            if self.model_type in ['xlnet']:\n",
        "                tokens = tokens + [SEP] +  [CLS]\n",
        "            else:\n",
        "                tokens = [CLS] + tokens + [SEP]\n",
        "        return tokens"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ort7lHvgFik4"
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Nxo5SCJgwb"
      },
      "source": [
        "In this implementation, be carefull about 3 things :\n",
        "\n",
        "1. As we are not using RNN, we have to limit the sequence length to the model input size.\n",
        "2. Most of the models require special tokens placed at the beginning and end of the sequences.\n",
        "3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with `add_prefix_space` set to `True`.\n",
        "\n",
        "Below, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the HuggingFace documentation in each model section.\n",
        "\n",
        "`bert:       [CLS] + tokens + [SEP] + padding`\n",
        "\n",
        "`roberta:    [CLS] + prefix_space + tokens + [SEP] + padding`\n",
        "\n",
        "`distilbert: [CLS] + tokens + [SEP] + padding`\n",
        "\n",
        "`xlm:        [CLS] + tokens + [SEP] + padding`\n",
        "\n",
        "`xlnet:      padding + tokens + [SEP] + [CLS]`\n",
        "\n",
        "### Custom Numericalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB7VJoNoJ7HF"
      },
      "source": [
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
        "    \n",
        "    def __getstate__(self):\n",
        "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
        "\n",
        "    def __setstate__(self, state:dict):\n",
        "        self.itos = state['itos']\n",
        "        self.tokenizer = state['tokenizer']\n",
        "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D__1A1C7ksnM"
      },
      "source": [
        "### Custom processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7zTXr5Jkuo9"
      },
      "source": [
        "transformer_vocab = TransformersVocab(tokenizer = transformer_tokenizer)\n",
        "numericalize_processor = NumericalizeProcessor(vocab = transformer_vocab)\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos = False, include_eos = False)\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chjKAN1vl14k"
      },
      "source": [
        "## Setting up the Databunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-jmHRp4l5Si"
      },
      "source": [
        "pad_first = bool(model_type in ['xlnet'])\n",
        "pad_idx = transformer_tokenizer.pad_token_id"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk54ORSPpgU0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(basic_tweet, test_size=0.3, random_state=42)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "S_leu-wSlzsu",
        "outputId": "3f094d37-350c-4d4f-cfb6-541e25b858d7"
      },
      "source": [
        "databunch = (TextList.from_df(train, cols='review', processor=transformer_processor)\n",
        "             .split_by_rand_pct(0.1,seed=seed)\n",
        "             .label_from_df(cols= 'sentiment')\n",
        "             .add_test(test)\n",
        "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:299: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNZhJ-jlqBRC"
      },
      "source": [
        "Check batch and tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "AN9RZye6pzx6",
        "outputId": "09a9c507-b129-4b5c-957a-34462d415adc"
      },
      "source": [
        "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
        "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
        "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
        "databunch.show_batch()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] token : [CLS]\n",
            "[SEP] token : [SEP]\n",
            "[PAD] token : [PAD]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[CLS] within the realm of science fiction , two particular themes consistently eli ##cit interest , were initially explored in the literature of a pre - cinematic era , and have since been periodically revisited by filmmakers and writers alike , with varying degrees of success . the first theme , that of time travel , has held an un ##wave ##ring fascination for fans of film , as well</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] despite its many faults , hallmark ' s 1995 version of gu ##lli ##ver ' s travels is still the finest adaptation of jonathan swift ' s satirical classic - largely because it not only includes all of gu ##lli ##ver ' s many travels but also includes the satire that ' s often overlooked . unfortunately the twin problems of the book ' s highly ep ##iso ##dic</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] i first saw this film by chance when i was visiting my uncle in arizona about 3 and 1 / 2 years ago . the vhs print was a little faded looking , but i was very haunted by what i had watched . did it all make sense ? well , honestly , no it didn ' t . however , this is a film that requires more</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] as a big fan of brian yu ##z ##na and the majority of the movies he ' s been involved in , i guessed i ' d enjoy pro ##geny . i didn ' t , although in ways it has it ' s moments . however , if you ' re expecting something of the cal ##ib ##re of society or beyond re ##ani ##mat ##or , you</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] this movie was awful , especially considering the work that must have gone into its production . though it ' s not as bad as ax ' em , it is quite awful . take into account the obvious rip - offs from glad ##ia ##tor and raiders of the lost ark , and what do you get ? this sm ##org ##as ##bor ##d of awful make -</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYwqTrtAqDMJ"
      },
      "source": [
        "Check batch and numericalizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgJsyx-aqFC5",
        "outputId": "8640f908-881f-4e47-80c7-82bd88a1d6e6"
      },
      "source": [
        "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
        "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
        "print('[PAD] id :', pad_idx)\n",
        "test_one_batch = databunch.one_batch()[0]\n",
        "print('Batch shape : ',test_one_batch.shape)\n",
        "print(test_one_batch)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] id : 101\n",
            "[SEP] id : 102\n",
            "[PAD] id : 0\n",
            "Batch shape :  torch.Size([16, 512])\n",
            "tensor([[  101,  2123,  1005,  ...,  1012,  1999,   102],\n",
            "        [  101,  1996, 10576,  ...,  1996,  3239,   102],\n",
            "        [  101,  2045,  2031,  ..., 12436, 28168,   102],\n",
            "        ...,\n",
            "        [  101,  1008,  1008,  ...,  1997,  2014,   102],\n",
            "        [  101,  1045,  2031,  ...,  1997,  2204,   102],\n",
            "        [  101,  2048, 28466,  ..., 25764, 13954,   102]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfvFEx98qHmi"
      },
      "source": [
        "### Custom model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqjm9zsNqQHG"
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        # attention_mask\n",
        "        # Mask to avoid performing attention on padding token indices.\n",
        "        # Mask values selected in ``[0, 1]``:\n",
        "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                  attention_mask = attention_mask)[0]   \n",
        "        return logits"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BrsXeRyrfEb",
        "outputId": "88caf833-bd73-44e1-a980-9a483bc115d8"
      },
      "source": [
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "config.num_labels = len(basic_tweet.sentiment.unique())\n",
        "print(len(basic_tweet.sentiment.unique()))\n",
        "config.use_bfloat16 = use_fp16\n",
        "print(config)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"dim\": 768,\n",
            "  \"do_sample\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"temperature\": 1.0,\n",
            "  \"tie_weights_\": true,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ4CJs9trpFm"
      },
      "source": [
        "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
        "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27A9Qcb7sEAE"
      },
      "source": [
        "## Learner : Custom Optimizer / Custom Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-IaWC5sFIC"
      },
      "source": [
        "from transformers import AdamW\n",
        "from functools import partial\n",
        "\n",
        "CustomAdamW = partial(AdamW, correct_bias=False)\n",
        "\n",
        "learner = Learner(databunch, \n",
        "                  custom_transformer_model, \n",
        "                  opt_func = CustomAdamW, \n",
        "                  metrics=[accuracy, error_rate])#.to_fp16()\n",
        "\n",
        "# Show graph of learner stats and metrics after each epoch.\n",
        "learner.callbacks.append(ShowGraph(learner))\n",
        "\n",
        "if use_fp16: learner = learner.to_fp16()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgLjlP-3t6ct",
        "outputId": "38d27116-ba27-42a9-c563-0f36bb8c2a0e"
      },
      "source": [
        "print(learner.model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomTransformerModel(\n",
            "  (transformer): BertForSequenceClassification(\n",
            "    (bert): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDGaKcUtsiAU"
      },
      "source": [
        "# list_layers = [learner.model.transformer.roberta.embeddings,\n",
        "#               learner.model.transformer.roberta.encoder.layer[0],\n",
        "#               learner.model.transformer.roberta.encoder.layer[1],\n",
        "#               learner.model.transformer.roberta.encoder.layer[2],\n",
        "#               learner.model.transformer.roberta.encoder.layer[3],\n",
        "#               learner.model.transformer.roberta.encoder.layer[4],\n",
        "#               learner.model.transformer.roberta.encoder.layer[5],\n",
        "#               learner.model.transformer.roberta.encoder.layer[6],\n",
        "#               learner.model.transformer.roberta.encoder.layer[7],\n",
        "#               learner.model.transformer.roberta.encoder.layer[8],\n",
        "#               learner.model.transformer.roberta.encoder.layer[9],\n",
        "#               learner.model.transformer.roberta.encoder.layer[10],\n",
        "#               learner.model.transformer.roberta.encoder.layer[11],\n",
        "#               learner.model.transformer.roberta.pooler]\n",
        "# learner.split(list_layers)\n",
        "learner.freeze_to(-1)\n",
        "# learner.summary()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "rHjymr62xMX9",
        "outputId": "37622d99-cc41-4f0a-9ff1-9f412d3f40ab"
      },
      "source": [
        "learner.unfreeze()\n",
        "learner.lr_find()\n",
        "learner.recorder.plot(skip_end=10,suggestion=True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      33.33% [1/3 00:19<00:38]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.684942</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='35' class='' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      89.74% [35/39 00:16<00:01 2.0154]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
            "Min numerical gradient: 1.10E-02\n",
            "Min loss divided by 10: 3.63E-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "hneR7OxyvkF4",
        "outputId": "57baffe1-2e81-40d4-b85c-93c4c09c5e14"
      },
      "source": [
        "learner.unfreeze()\n",
        "# learner.freeze_to(-2)\n",
        "learner.fit_one_cycle(3,max_lr=2e-03)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.982689</td>\n",
              "      <td>0.686239</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.787405</td>\n",
              "      <td>0.686270</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.731014</td>\n",
              "      <td>0.692489</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4IYA8neuOCY",
        "outputId": "bcc5d7c1-87cf-497d-eabd-c781dfa67235"
      },
      "source": [
        "learner.predict('this movie is boring')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category negative, tensor(0), tensor([0.5023, 0.4977]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caabPLglulcf"
      },
      "source": [
        "learner.export(file = os.path.join(path, 'transformer.pkl'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "J1RME5b1vZl7",
        "outputId": "3ba41d7a-e2a3-4a35-deb2-c790ff7abb85"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    the get_preds method does not yield the elements in order by default\n",
        "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
        "    \"\"\"\n",
        "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    return preds[reverse_sampler, :]\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acj-mVI2v0wy",
        "outputId": "f122df8e-6ed4-4939-ef77-f732b49e6981"
      },
      "source": [
        "test_preds"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659],\n",
              "       ...,\n",
              "       [0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659],\n",
              "       [0.50234 , 0.497659]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHSo49vDv7Kq"
      },
      "source": [
        "test['prediction'] = np.argmax(test_preds,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqARHfvkv38S"
      },
      "source": [
        "test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVaABHoxNZg"
      },
      "source": [
        "np.argmax(test_preds,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}