{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71f12ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('C:\\\\Users\\\\luoyan011\\\\Desktop\\\\PersonalLearning\\\\GitHub\\\\python_functions\\\\jl_nlp_pkg')\n",
    "sys.path.append('C:\\\\Users\\\\luoyan011\\\\Desktop\\\\PersonalLearning\\\\GitHub\\\\python_functions\\\\jl_model_explain_pkg')\n",
    "import nlpbasic.textClean as textClean\n",
    "import nlpbasic.docVectors as DocVector\n",
    "import nlpbasic.dataExploration as DataExploration\n",
    "import nlpbasic.lda as lda\n",
    "import nlpbasic.tfidf as tfidf\n",
    "import nlpbasic.text_summarize as txtsmr\n",
    "import nlpbasic.word_embedding as wdembd\n",
    "\n",
    "import model_explain.plot as meplot\n",
    "import model_explain.shap as meshap\n",
    "\n",
    "import data_visualization.distribution_plot as dbplot\n",
    "from numpy import array,asarray,zeros\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding\n",
    "\n",
    "datapath = 'C:\\\\Users\\\\luoyan011\\\\Desktop\\\\PersonalLearning\\\\GitHub\\\\NLP_data'\n",
    "datapath2 = 'C:\\\\Users\\\\luoyan011\\\\Desktop\\\\PersonalLearning\\\\GitHub\\\\NLP_data\\\\ATIS'\n",
    "#dataset: https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem?select=atis_intents_train.csv\n",
    "#dataset2: https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e378925",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efced77f",
   "metadata": {},
   "source": [
    "**Chatboat**: https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d3cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    line = re.sub(r'[^a-zA-z.?!\\']', ' ', line)\n",
    "    line = re.sub(r'[ ]+', ' ', line)\n",
    "    return line\n",
    "with open(os.path.join(datapath, 'Intent.json')) as f:\n",
    "          intents = json.load(f)\n",
    "\n",
    "# get text and intent title from json data: output is a dictionary\n",
    "inputs, targets = [], []\n",
    "classes = []\n",
    "intent_doc = {}\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    if intent['intent'] not in classes:\n",
    "        classes.append(intent['intent'])\n",
    "    if intent['intent'] not in intent_doc:\n",
    "        intent_doc[intent['intent']] = []\n",
    "        \n",
    "    for text in intent['text']:\n",
    "        inputs.append(preprocessing(text))\n",
    "        targets.append(intent['intent'])\n",
    "        \n",
    "    for response in intent['responses']:\n",
    "        intent_doc[intent['intent']].append(response)\n",
    "        \n",
    "#generate dataset\n",
    "data = intents['intents']\n",
    "dataset = pd.DataFrame(columns=['intent', 'text', 'response'])\n",
    "for i in data:\n",
    "    intent = i['intent']\n",
    "    for t, r in zip(i['text'], i['responses']):\n",
    "        row = {'intent': intent, 'text': t, 'response':r}\n",
    "        dataset = dataset.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1919b832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.intent.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a76eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x in dataset.text]\n",
    "y = pd.get_dummies(dataset.intent).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "083549d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "embedding_dim = 100 \n",
    "# download glove word embedding first and then load it with the following code\n",
    "f = open('C:/ProgramData/Anaconda3/append_file/glove/glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close\n",
    "print('loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "896e58bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding:\n",
      " [25]\n",
      "\n",
      "Word Indices:\n",
      " [('hi', 25)]\n",
      "vocab size: 96\n"
     ]
    }
   ],
   "source": [
    "max_length = int(np.percentile(dataset.text.apply(lambda x: len(x.split())), 95))\n",
    "# we also tried max length, but it cause overfitting\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "# print(\"words with freq:\", t.word_docs)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs = t.texts_to_sequences(X)\n",
    "print('Encoding:\\n', encoded_docs[0])\n",
    "print('\\nWord Indices:\\n', [(t.index_word[i], i) for i in encoded_docs[0]])\n",
    "print('vocab size:', vocab_size)\n",
    "train_padded_docs = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d12eb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, embedding_dim))\n",
    "for word, idx_word in t.word_index.items():\n",
    "    word_vector = embeddings_index.get(word)\n",
    "    if word_vector is not None:\n",
    "        embedding_matrix[idx_word] = word_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "feeebd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 100)            9600      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 100)               70100     \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 22)                2222      \n",
      "=================================================================\n",
      "Total params: 81,922\n",
      "Trainable params: 72,322\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = max_length, trainable = False),\n",
    "        Flatten(),\n",
    "        Dense(embedding_dim, activation=\"relu\", name=\"layer1\"),\n",
    "        Dense(22, activation = 'softmax', name=\"layer2\")\n",
    "        \n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "952fc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 10ms/step - loss: 3.1228 - acc: 0.0617\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 2.6270 - acc: 0.3704\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3016 - acc: 0.5556\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0268 - acc: 0.6296\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.7779 - acc: 0.6914\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5604 - acc: 0.7654\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3626 - acc: 0.8642\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.1923 - acc: 0.8765\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0479 - acc: 0.8889\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9140 - acc: 0.8889\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded_docs, y, epochs = 10, verbose = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4d9fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gossip'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = pd.get_dummies(dataset.intent).columns.tolist()\n",
    "encoded_val_doc = t.texts_to_sequences(['You'])\n",
    "padded_val_doc = pad_sequences(encoded_val_doc, maxlen = max_length, padding = 'post')\n",
    "label_list[np.argmax(model.predict(padded_val_doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "639f97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Enter 'quit' to break the loop.\n",
      "You: you\n",
      "Bot: Crystal said she listen to me the least I could do for him is listen to him. -- TYPE: Gossip\n",
      "\n",
      "You: How are you\n",
      "Bot: Hello, I am good thank you, how are you? Please tell me your GeniSys user -- TYPE: CourtesyGreeting\n",
      "\n",
      "You: Who are you\n",
      "Bot: Your name is <HUMAN>, how can I help you? -- TYPE: CurrentHumanQuery\n",
      "\n",
      "You: I want to know\n",
      "Bot: A snake slithers into a pub and up to the bar. The landlord says, 'I'm sorry, but I can't serve you.' 'What? Why not?' asks the snake. 'Because', says the landlord, 'You can't hold your drink.' -- TYPE: Jokes\n",
      "\n",
      "You: What's your name\n",
      "Bot: Chaos said he may very well buy I soon if only to support dr wallace's work. -- TYPE: Gossip\n",
      "\n",
      "You: what's your name?\n",
      "Bot: Her_again said she watch whose line is it anyway whenever he is home and it is on. -- TYPE: Gossip\n",
      "\n",
      "You: can you open the door?\n",
      "Bot: Jerry said I meant that as far as I can tell my emotions are real to me. -- TYPE: Gossip\n",
      "\n",
      "You: can you tell me if you are self-aware?\n",
      "Bot: That is an difficult question, can you prove that you are? -- TYPE: SelfAware\n",
      "\n",
      "You: tell me a joke\n",
      "Bot: Descartes walks into a pub. 'Would you like a beer sir?' asks the landlord politely. Descartes replies, 'I think not' and ping! he vanishes. -- TYPE: Jokes\n",
      "\n",
      "You: do you like beer?\n",
      "Bot: David said he lost his paper on I when his dad was cleaning up his room. -- TYPE: Gossip\n",
      "\n",
      "You: why you are so silly\n",
      "Bot: Hello, how are you? I am great thanks! Please tell me your GeniSys user -- TYPE: CourtesyGreeting\n",
      "\n",
      "You: i dont like you\n",
      "Bot: Jackie said I explained to him already well enough further questions are hard to make on the subject. -- TYPE: Gossip\n",
      "\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "label_list = pd.get_dummies(dataset.intent).columns.tolist()\n",
    "def response(sentence, maxlen):\n",
    "    encoded_val_doc = t.texts_to_sequences([sentence])\n",
    "    padded_val_doc = pad_sequences(encoded_val_doc, maxlen = maxlen, padding = 'post')\n",
    "\n",
    "    # predict the category of input sentences\n",
    "    pred_class = label_list[np.argmax(model.predict(padded_val_doc))]\n",
    "    \n",
    "    # choice a random response for predicted sentence\n",
    "    return random.choice(intent_doc[pred_class]), pred_class\n",
    "\n",
    "# chat with bot\n",
    "print(\"Note: Enter 'quit' to break the loop.\")\n",
    "while True:\n",
    "    input_ = input('You: ')\n",
    "    if input_.lower() == 'quit':\n",
    "        break\n",
    "    res, typ = response(input_, max_length)\n",
    "    print('Bot: {} -- TYPE: {}'.format(res, typ))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1eb369",
   "metadata": {},
   "source": [
    "**Introducing ATIS : Intent Classification Dataset**: https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd2dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_train = pd.read_csv(os.path.join(datapath2, 'atis_intents_train.csv'),header=None).rename(columns = {0: 'intent', 1: 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5236fa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             intent                                               text\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what is the arrival time in san francisco for..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atis_train.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
