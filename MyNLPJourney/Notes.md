## [Modern Approach in NLP Processing](https://compstat-lmu.github.io/seminar_nlp_ss20/introduction-deep-learning-for-nlp.html)
- Introduction: DL for NLP
  - Word Embedding and Nueral Netword Model
    - Word Embedding
      - Word embeddings use dense vector representations for words. That means they map each word to a continuous vector with n dimensions. The distance in the vector space denotes semantic (dis)similarity.
      - Practically all NLP projects these days build upon word embeddings, since they have a lot of advantages compared to the aforementioned representations
      - Approach:
        - Word2Vec(Continous Bag-Of-Words (CBOW) and Skip-gram) 
        - Glove
    - RNN: Recurrent neural networks or RNNs are a special family of neural networks which were explicitely developed for modeling sequential data like text. RNNs process a sequence of words or letters by going through its elements one by one and capturing information based on the previous elements.One particular reason why recurrent networks have become such a powerful technique in processing sequential data is parameter sharing.
    ![image](https://user-images.githubusercontent.com/16402963/119266494-0b6d7a00-bbb9-11eb-81a5-f36791fd714a.png)
    - CNN: convolutional neural networks (CNN) are widely used in computer vision, utilizing CNN to word embedding matrices and automatically extract features to handle NLP tasks appeared inevitable
    - 
- 
