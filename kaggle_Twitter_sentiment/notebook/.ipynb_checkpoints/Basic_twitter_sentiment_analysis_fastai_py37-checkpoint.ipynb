{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131e602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from numpy import array,asarray,zeros\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "root_path = 'C:\\\\Users\\\\luoyan011\\\\Desktop\\\\PersonalLearning\\\\GitHub\\\\NLP_data'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f6f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_emojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "def tweet_proc(df, text_col='text'):\n",
    "    df['orig_text'] = df[text_col]\n",
    "    # Remove twitter handles\n",
    "    df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    # Remove URLs\n",
    "    df[text_col] = df[text_col].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Remove emojis\n",
    "    df[text_col] = df[text_col].apply(de_emojify)\n",
    "    # Remove hashtags\n",
    "    df[text_col] = df[text_col].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "    return df[df[text_col]!='']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2db28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.300000e+18</td>\n",
       "      <td>RT  91-year-old Ex-Vice President Moody Awori Lands Inter County Covid-19 Committee Role</td>\n",
       "      <td>neu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.300000e+18</td>\n",
       "      <td>RT  BREAKING: The Department of Health reports 4,339 more people caught COVID-19, pushing the national case count to 178,02</td>\n",
       "      <td>neu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.300000e+18</td>\n",
       "      <td>RT   Helps Out Fan Who Requested Him To Help Arrange A Bed For Her  Positive Father (View Tweet)\\n</td>\n",
       "      <td>pos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.300000e+18   \n",
       "1  1.300000e+18   \n",
       "2  1.300000e+18   \n",
       "\n",
       "                                                                                                                          text  \\\n",
       "0                                    RT  91-year-old Ex-Vice President Moody Awori Lands Inter County Covid-19 Committee Role    \n",
       "1  RT  BREAKING: The Department of Health reports 4,339 more people caught COVID-19, pushing the national case count to 178,02   \n",
       "2                           RT   Helps Out Fan Who Requested Him To Help Arrange A Bed For Her  Positive Father (View Tweet)\\n   \n",
       "\n",
       "  sentiment  label  \n",
       "0       neu    NaN  \n",
       "1       neu    NaN  \n",
       "2       pos    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_tweet = pd.read_csv(os.path.join(root_path, \"Covid-19 Twitter Dataset (Aug-Sep 2020).csv\"))\n",
    "covid_tweet = covid_tweet[covid_tweet.original_text.isnull()==False].drop_duplicates().reset_index(drop=True)\n",
    "covid_tweet = tweet_proc(covid_tweet,'original_text')\n",
    "covid_tweet['label'] = np.nan\n",
    "covid_tweet = covid_tweet[covid_tweet.lang=='en']\n",
    "covid_tweet = covid_tweet[['id', 'original_text', 'sentiment', 'label']].rename(columns={'original_text':'text'})\n",
    "covid_tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb699cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "0  1467810369   \n",
       "1  1467810672   \n",
       "2  1467810917   \n",
       "\n",
       "                                                                                                              text  \\\n",
       "0                                   - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D   \n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   \n",
       "2                                  I dived many times for the ball. Managed to save 50%  The rest go out of bounds   \n",
       "\n",
       "  sentiment  label  \n",
       "0       neg      0  \n",
       "1       neg      0  \n",
       "2       neg      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tweet = pd.read_csv(os.path.join(root_path, \"sentiment140_twitter.csv\"), names=['target', 'id', 'date', 'flag', 'user', 'text'], header=None,encoding = \"ISO-8859-1\")\n",
    "basic_tweet = basic_tweet[basic_tweet.text.isnull()==False].drop_duplicates().reset_index(drop=True)\n",
    "basic_tweet = tweet_proc(basic_tweet,'text')\n",
    "basic_tweet['label'] = np.where(basic_tweet['target']==0, 0, 1)\n",
    "basic_tweet['sentiment'] = np.where(basic_tweet['target']==0, 'neg', 'pos')\n",
    "basic_tweet = basic_tweet[['id', 'text', 'sentiment', 'label']]\n",
    "basic_tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5870d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842736 1600000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              text  \\\n",
       "0                                   - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D   \n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   \n",
       "2                                  I dived many times for the ball. Managed to save 50%  The rest go out of bounds   \n",
       "\n",
       "   label  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lm = basic_tweet.append(covid_tweet)\n",
    "df_clas = df_lm[['text', 'label']].dropna(subset=['label'])\n",
    "print(len(df_lm), len(df_clas))\n",
    "df_clas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a3a7d",
   "metadata": {},
   "source": [
    "# DL & Transfer Learning with fastai\n",
    "some reference:\n",
    "\n",
    "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta\n",
    "\n",
    "https://www.kaggle.com/twhelan/covid-19-vaccine-sentiment-analysis-with-fastai\n",
    "\n",
    "https://www.youtube.com/watch?v=WjnwWeGjZcM&t=626s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5405bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `n_workers` has to be changed to 0 to avoid getting stuck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\luoyan011\\.conda\\envs\\py37\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "dls_lm = TextDataLoaders.from_df(df_lm, text_col='text', is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c815e9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i missed this .. not sure how i did nt know about this xxbos xxmaj ah cool … wow .. i know where you are ! ! xxmaj astoria looks really neat . xxmaj let me know what you think . xxmaj too bad we missed each other . so close ! ! xxbos xxmaj morning , xxmaj i 'm fine ta except diet starts today &amp; &amp; xxmaj i 'm</td>\n",
       "      <td>i missed this .. not sure how i did nt know about this xxbos xxmaj ah cool … wow .. i know where you are ! ! xxmaj astoria looks really neat . xxmaj let me know what you think . xxmaj too bad we missed each other . so close ! ! xxbos xxmaj morning , xxmaj i 'm fine ta except diet starts today &amp; &amp; xxmaj i 'm craving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>personal assistant xxbos going to gourock pool soon xxrep 3 x xxbos i love the new xxup kendra show xxbos xxmaj i 'll be in xxmaj iloilo at the time of the concert ; do n't know that xxmaj i 'll be done with the reason for the trip by then . xxmaj doubt it ! xxbos xxmaj just about ready for church .. i ca n't find my shoes . xxbos</td>\n",
       "      <td>assistant xxbos going to gourock pool soon xxrep 3 x xxbos i love the new xxup kendra show xxbos xxmaj i 'll be in xxmaj iloilo at the time of the concert ; do n't know that xxmaj i 'll be done with the reason for the trip by then . xxmaj doubt it ! xxbos xxmaj just about ready for church .. i ca n't find my shoes . xxbos i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c69a99",
   "metadata": {},
   "source": [
    "xxbox means the next word is the first word of the sentence, xxmaj means the next word start with capital string, xxrep followed with numbers means the next word has been repeated for n times where n = number, xxup means the next word is in capital. in fastai, the max_vocab is set as 60,000, which results in fastai replacing all words other than the most common 60,000 with a special unknow word token xxunk. which can avoid an overly large embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bd1c7",
   "metadata": {},
   "source": [
    "### Fine-tuning the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f389f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult = 0.3, metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b48c6",
   "metadata": {},
   "source": [
    "Here we passed language_model_learner our DataLoaders, dls_lm, and the pre-trained RNN model, AWD_LSTM, which is built into fastai. drop_mult is a multiplier applied to all dropouts in the AWD_LSTM model to reduce overfitting. For example, by default fastai's AWD_LSTM applies EmbeddingDropout with 10% probability (at the time of writing), but we told fastai that we want to reduce that to 3%. The metrics we want to track are perplexity, which is the exponential of the loss (in this case cross entropy loss), and accuracy, which tells us how often our model predicts the next word correctly. We can also train with fp16 to use less memory and speed up the training process.\n",
    "\n",
    "We can find a good learning rate for training using lr_find and use that to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9155e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d1d84",
   "metadata": {},
   "source": [
    "When we created our Learner the embeddings from the pre-trained AWD_LSTM model were merged with random embeddings added for words that weren't in the vocabulary. The pre-trained layers were also automatically frozen for us. Using fit_one_cycle with our Learner will train only the new random embeddings (i.e. words that are in our Twitter vocab but not the Wikipedia vocab) in the last layer of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c09227",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f902a",
   "metadata": {},
   "source": [
    "We can unfreeze the entire model, find a more suitable learning rate and train for a few more epochs to improve the accuracy further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ed931",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, 1e-3) #4 means 4 epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d75473",
   "metadata": {},
   "source": [
    "We can test our model by predicting the next word as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464afb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I love\"\n",
    "N_WORDS = 30\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125c0d5",
   "metadata": {},
   "source": [
    "Let's save the model encoder so we can use it to fine-tune our classifier. The encoder is all of the model except for the final layer, which converts activations to probabilities of picking each token in the vocabulary. We want to keep the knowledge the model has learned about tweet language but we won't be using our classifier to predict the next word in a sentence, so we won't need the final layer any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned_lm') # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbf3b7",
   "metadata": {},
   "source": [
    "### Training a sentiment classifier\n",
    "To get the `DataLoaders` for our classifier let's use the `DataBlock` API this time, which is more customisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f87bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `n_workers` has to be changed to 0 to avoid getting stuck\n"
     ]
    }
   ],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks = (TextBlock.from_df('text', seq_len = dls_lm.seq_len, vocab = dls_lm.vocab), CategoryBlock),\n",
    "    get_x = ColReader('text'),\n",
    "    get_y = ColReader('label'),\n",
    "    splitter = RandomSplitter()\n",
    ").dataloaders(df_clas, bs = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff41f0",
   "metadata": {},
   "source": [
    "To use the API, fastai needs the following:\n",
    "\n",
    "- `blocks`:\n",
    "    - `TextBlock`: Our x variable will be text contained in a pandas DataFrame. We want to use the same sequence length and vocab as the language model DataLoaders so we can make use of our pre-trained model.\n",
    "    - `CategoryBlock`: Our y variable will be a single-label category (negative, neutral or positive sentiment).\n",
    "    - `get_x`, `get_y`: Get data for the model by reading the text and sentiment columns from the DataFrame.\n",
    "    - `splitter`: We will use `RandomSplitter()` to randomly split the data into a training set (80% by default) and a validation set (20%).\n",
    "    - `dataloaders`: Builds the `DataLoaders` using the DataBlock template we just defined, the df_clas DataFrame and a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ac377",
   "metadata": {},
   "source": [
    "Initialising the `Learner` is similar to before, but in this case we want a `text_classifier_learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4594f4",
   "metadata": {},
   "source": [
    "Finally, we want to load the encoder from the language model we trained earlier, so our classifier uses pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e79630",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned_lm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75203315",
   "metadata": {},
   "source": [
    "#### Fine-tuning the classifier\n",
    "Now we can train the classifier using discriminative learning rates and gradual unfreezing, which has been found to give better results for this type of model. First let's freeze all but the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64ecef",
   "metadata": {},
   "source": [
    "Now freeze all but the last two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9908288",
   "metadata": {},
   "source": [
    "Now all but the last three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7424dd9",
   "metadata": {},
   "source": [
    "Finally, let's unfreeze the entire model and train a bit more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49726321",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab28f94",
   "metadata": {},
   "source": [
    "### Analysis the tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
