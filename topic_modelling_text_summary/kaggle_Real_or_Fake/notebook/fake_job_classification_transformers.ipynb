{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_job_classification_transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0740f36209254410981b145cbc7cedd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d2540cec4504b7a86476668f30145cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be40447290cf49a6926707c47f43d0ef",
              "IPY_MODEL_7cb532e94e3e4c3aae1f0b585a0d7c45"
            ]
          }
        },
        "4d2540cec4504b7a86476668f30145cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be40447290cf49a6926707c47f43d0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b83cd2056e82404b8e242a59127a2ffa",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08bb1735d1e7457c981e301904bd175b"
          }
        },
        "7cb532e94e3e4c3aae1f0b585a0d7c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c5448f20dcac4e1293f8b6f1901e2e2a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [1:40:57&lt;00:00, 1514.34s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1d33f7178394eafb7fb64f9b2af1784"
          }
        },
        "b83cd2056e82404b8e242a59127a2ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08bb1735d1e7457c981e301904bd175b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5448f20dcac4e1293f8b6f1901e2e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1d33f7178394eafb7fb64f9b2af1784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v9CNJvjsz58",
        "outputId": "27b4caad-681c-43b4-94a1-3e802f4b4b72"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "!pip install transformers\n",
        "!pip install SentencePiece\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from pylab import rcParams\n",
        "\n",
        "from torch import nn, optim\n",
        "from tqdm.notebook import tqdm\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import (AutoConfig, \n",
        "                          AutoModelForSequenceClassification, \n",
        "                          AutoTokenizer, AdamW, \n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          set_seed,\n",
        "                          )\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 20.5MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
            "Collecting SentencePiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.96\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "cYfZDDKdtI9w",
        "outputId": "9076eb65-9664-4aea-ab37-8833f5346e0d"
      },
      "source": [
        "path = '/content/drive/MyDrive/colab_data'\n",
        "train = pd.read_csv(os.path.join(path, \"fake_job_postings.csv\"))\n",
        "train['jd'] = train[['title','function','employment_type','company_profile','description','requirements','benefits']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "df_train, df_val = train_test_split(train, test_size=0.33, random_state=42)\n",
        "print(np.quantile(df_train.jd.apply(lambda x: len(x)),0.95))\n",
        "df_train.head(3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5365.500000000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_id</th>\n",
              "      <th>title</th>\n",
              "      <th>location</th>\n",
              "      <th>department</th>\n",
              "      <th>salary_range</th>\n",
              "      <th>company_profile</th>\n",
              "      <th>description</th>\n",
              "      <th>requirements</th>\n",
              "      <th>benefits</th>\n",
              "      <th>telecommuting</th>\n",
              "      <th>has_company_logo</th>\n",
              "      <th>has_questions</th>\n",
              "      <th>employment_type</th>\n",
              "      <th>required_experience</th>\n",
              "      <th>required_education</th>\n",
              "      <th>industry</th>\n",
              "      <th>function</th>\n",
              "      <th>fraudulent</th>\n",
              "      <th>jd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13014</th>\n",
              "      <td>13015</td>\n",
              "      <td>Digital Senior Account Manager</td>\n",
              "      <td>GB, LND, London</td>\n",
              "      <td>Client Services</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Working in over 60 countries and 30 languages,...</td>\n",
              "      <td>Role SummaryAutotorq is looking for a self-mot...</td>\n",
              "      <td>Key Skills/Experience:Digital marketing experi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Director</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Automotive</td>\n",
              "      <td>Marketing</td>\n",
              "      <td>0</td>\n",
              "      <td>Digital Senior Account Manager Marketing Full-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12094</th>\n",
              "      <td>12095</td>\n",
              "      <td>Sales Rep for AT&amp;T Solutions Provider - Manage...</td>\n",
              "      <td>US, TX, Arlington</td>\n",
              "      <td>AFSDT</td>\n",
              "      <td>45000-67000</td>\n",
              "      <td>Argenta Field Solutions values the client, cre...</td>\n",
              "      <td>Interviewing now for full-time positions in Ar...</td>\n",
              "      <td>- Sales experience preferred- Ability to work ...</td>\n",
              "      <td>- AFLAC- Health Insurance (Management) - Train...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consumer Services</td>\n",
              "      <td>Sales</td>\n",
              "      <td>0</td>\n",
              "      <td>Sales Rep for AT&amp;T Solutions Provider - Manage...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16090</th>\n",
              "      <td>16091</td>\n",
              "      <td>Investor Development Analyst</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MarketInvoice is one of the most high-profile ...</td>\n",
              "      <td>MarketInvoice is one of the most high-profile ...</td>\n",
              "      <td>Experience equivalent to a minimum of 2 years ...</td>\n",
              "      <td>Full time role based in our London Holborn off...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Associate</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Financial Services</td>\n",
              "      <td>Business Development</td>\n",
              "      <td>0</td>\n",
              "      <td>Investor Development Analyst Business Developm...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       job_id  ...                                                 jd\n",
              "13014   13015  ...  Digital Senior Account Manager Marketing Full-...\n",
              "12094   12095  ...  Sales Rep for AT&T Solutions Provider - Manage...\n",
              "16090   16091  ...  Investor Development Analyst Business Developm...\n",
              "\n",
              "[3 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_LBXPWMtuUP"
      },
      "source": [
        "# model_name = 'bert-base-cased'\n",
        "# model_name = 'xlnet-base-cased'\n",
        "# model_name = 'xlm-roberta-large'\n",
        "model_name = 'bert-base-uncased'\n",
        "n_labels = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch7r-u8buSMB",
        "outputId": "4a3ec368-b1ec-4cab-fd91-9d3da8d41e24"
      },
      "source": [
        "class Transformer_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, text, target, tokenizer, max_len):\n",
        "        self.text = text\n",
        "        self.target = target\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.text[item])\n",
        "        target = self.target[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=False,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'], maxlen=max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype = 'int64')\n",
        "        input_ids = torch.tensor(input_ids) \n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "        attention_mask = torch.tensor(attention_mask)       \n",
        "\n",
        "        return {\n",
        "        'text': text,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask.flatten(),\n",
        "        'target': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(df, text, target, tokenizer, max_len, batch_size):\n",
        "  ds = Transformer_Dataset(\n",
        "    text=df[text].to_numpy(),\n",
        "    target=df[target].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2,\n",
        "    drop_last = True\n",
        "  )\n",
        "\n",
        "# Get model configuration.\n",
        "print('Loading configuraiton...')\n",
        "model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path=model_name, \n",
        "                                          num_labels=n_labels)\n",
        "\n",
        "# Get model's tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
        "\n",
        "# Get the actual model.\n",
        "print('Loading model...')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name, \n",
        "                                                           config=model_config)\n",
        "\n",
        "# Load model to defined device.\n",
        "model.to(device)\n",
        "print('Model loaded to `%s`'%device)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading configuraiton...\n",
            "Loading tokenizer...\n",
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded to `cuda`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_prsn-uVHR"
      },
      "source": [
        "set_seed(123)\n",
        "epochs = 4\n",
        "batches = 10\n",
        "max_len = 512\n",
        "class_names = ['no_fraud','fraud']\n",
        "output_model = 'research_cat.bin'\n",
        "train_data_loader = create_data_loader(df_train,'jd', 'fraudulent', tokenizer, max_len, batches)\n",
        "val_data_loader = create_data_loader(df_val, 'jd', 'fraudulent', tokenizer, max_len, batches)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2QuDbdubuk"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
        "\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhvnhNWDueut"
      },
      "source": [
        "from sklearn import metrics\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples, batch_size, maxlen):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "  \n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].reshape(batch_size,maxlen).to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        target = d['target'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = target)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # preds = preds.cpu().detach().numpy()\n",
        "        _, prediction = torch.max(outputs[1], dim=1)\n",
        "        target = target.cpu().detach().numpy()\n",
        "        prediction = prediction.cpu().detach().numpy()\n",
        "        accuracy = metrics.accuracy_score(target, prediction)\n",
        "\n",
        "        acc += accuracy\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        counter = counter + 1\n",
        "\n",
        "    return acc / counter, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device, n_examples,batch_size,maxlen):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].reshape(batch_size,maxlen).to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            target = d['target'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = target)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "            _, prediction = torch.max(outputs[1], dim=1)\n",
        "            target = target.cpu().detach().numpy()\n",
        "            prediction = prediction.cpu().detach().numpy()\n",
        "            accuracy = metrics.accuracy_score(target, prediction)\n",
        "\n",
        "            acc += accuracy\n",
        "            losses.append(loss.item())\n",
        "            counter += 1\n",
        "\n",
        "    return acc / counter, np.mean(losses)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "0740f36209254410981b145cbc7cedd7",
            "4d2540cec4504b7a86476668f30145cf",
            "be40447290cf49a6926707c47f43d0ef",
            "7cb532e94e3e4c3aae1f0b585a0d7c45",
            "b83cd2056e82404b8e242a59127a2ffa",
            "08bb1735d1e7457c981e301904bd175b",
            "c5448f20dcac4e1293f8b6f1901e2e2a",
            "b1d33f7178394eafb7fb64f9b2af1784"
          ]
        },
        "id": "LXkF2tpjuijt",
        "outputId": "7aae636b-28a0-4c66-d87d-2c41a44200a7"
      },
      "source": [
        "\n",
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,     \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler, \n",
        "        len(df_train),\n",
        "        batches,\n",
        "        max_len\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader, \n",
        "        device, \n",
        "        len(df_val),\n",
        "        batches,\n",
        "        max_len\n",
        "    )\n",
        "\n",
        "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model.state_dict(), os.path.join(path, output_model))\n",
        "        best_accuracy = val_acc"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0740f36209254410981b145cbc7cedd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "----------\n",
            "Train loss 0.1801425774823703 Train accuracy 0.9572263993316592\n",
            "Val loss 0.11365080986456093 Val accuracy 0.9725423728813537\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "Train loss 0.10136969387035276 Train accuracy 0.9773600668337494\n",
            "Val loss 0.07805963823556016 Val accuracy 0.9813559322033882\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "Train loss 0.05327418409719746 Train accuracy 0.9895572263993311\n",
            "Val loss 0.06732603605901744 Val accuracy 0.9859322033898292\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "Train loss 0.026411552594499735 Train accuracy 0.9954887218045112\n",
            "Val loss 0.06847450527879488 Val accuracy 0.9876271186440665\n",
            "\n",
            "\n",
            "CPU times: user 1h 39min 28s, sys: 1min 5s, total: 1h 40min 34s\n",
            "Wall time: 1h 40min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbFXHrrbpVX"
      },
      "source": [
        "def predict_sentiment(text):\n",
        "    text = text\n",
        "\n",
        "    encoded_review = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_len,\n",
        "    return_token_type_ids=False,\n",
        "    pad_to_max_length=False,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = pad_sequences(encoded_review['input_ids'], maxlen=max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "    input_ids = input_ids.astype(dtype = 'int64')\n",
        "    input_ids = torch.tensor(input_ids) \n",
        "\n",
        "    attention_mask = pad_sequences(encoded_review['attention_mask'], maxlen=max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "    attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "    attention_mask = torch.tensor(attention_mask) \n",
        "\n",
        "    input_ids = input_ids.reshape(1,max_len).to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    outputs = outputs[0][0].cpu().detach()\n",
        "\n",
        "    probs = F.softmax(outputs, dim=-1).cpu().detach().numpy().tolist()\n",
        "    _, prediction = torch.max(outputs, dim =-1)\n",
        "    # return class_names[prediction]\n",
        "    return prediction\n",
        "    # print(\"Positive score:\", probs[1])\n",
        "    # print(\"Negative score:\", probs[0])\n",
        "    # print(\"Neutral score\", probs[2])\n",
        "    # print(f'text: {text}')\n",
        "    # print(f'target  : {class_names[prediction]}')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4FYrVDbsjk",
        "outputId": "e8c3c667-cc55-4d39-a70b-e0ff4dad6007"
      },
      "source": [
        "df_val['pred']=df_val.jd.apply(lambda x: int(predict_sentiment(x)))\n",
        "print(confusion_matrix(df_val.fraudulent, df_val.pred))\n",
        "print(classification_report(df_val.fraudulent, df_val.pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5581   21]\n",
            " [  52  247]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      5602\n",
            "           1       0.92      0.83      0.87       299\n",
            "\n",
            "    accuracy                           0.99      5901\n",
            "   macro avg       0.96      0.91      0.93      5901\n",
            "weighted avg       0.99      0.99      0.99      5901\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}