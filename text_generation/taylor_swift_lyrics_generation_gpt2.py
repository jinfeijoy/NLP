# -*- coding: utf-8 -*-
"""taylor_swift_lyrics_generation_gpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFtkFkrja-KK6M1q0DCn2mrWt7G4qNMV
"""

# Commented out IPython magic to ensure Python compatibility.
import csv
import io
import os
import re
import pandas as pd
# %tensorflow_version 1.x
!pip install -q gpt-2-simple
import gpt_2_simple as gpt2
from datetime import datetime
from google.colab import files
import numpy as np

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/colab_data'

gpt2.download_gpt2(model_name="124M")



dataset = pd.read_csv(os.path.join(path, "taylor_swift_lyrics.csv"), encoding= 'unicode_escape')

def processFirstLine(lyrics, songID, songName, row):
    lyrics.append(row['lyric'] + '.\n')
    songID.append( row['year']*100+ row['track_n'])
    songName.append(row['track_title'])
    return lyrics,songID,songName
# define empty lists for the lyrics , songID , songName 
lyrics = []
songID = []
songName = []

# songNumber indicates the song number in the dataset
songNumber = 1

# i indicates the song number
i = 0
isFirstLine = True

# Iterate through every lyrics line and join them together for each song independently 
for index,row in dataset.iterrows():
    if(songNumber == row['track_n']):
        if (isFirstLine):
            lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)
            isFirstLine = False
        else :
            #if we still in the same song , keep joining the lyrics lines    
            lyrics[i] +=  row['lyric'] + '.\n'
    #When it's done joining a song's lyrics lines , go to the next song :    
    else :
        lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)
        songNumber = row['track_n']
        i+=1

# Define a new pandas DataFrame to save songID , songName , Lyrics in it to use them later
lyrics_data = pd.DataFrame({'songID':songID, 'songName':songName, 'lyrics':lyrics })
lyrics_data.head(3)

with open('/content/drive/My Drive/lyrics.txt', 'w') as f:
    for item in lyrics_data.lyrics.to_list():
        f.write("%s\n" % item)

gpt2.copy_file_from_gdrive('lyrics.txt')

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset='lyrics.txt',
              model_name='124M',
              steps=1000,
              restore_from='fresh',
              run_name='run1',
              print_every=10,
              sample_every=200,
              save_every=500
              )

gpt2.generate(sess,
              length=250,
              temperature=0.7,
              prefix="My Love",
              nsamples=5,
              batch_size=5
              )